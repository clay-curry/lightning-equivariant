{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_l0tU8gum9eT"
      },
      "source": [
        "# A Gentle Introduction to Geometric Deep Learning with Lightning Equivariant\n",
        "\n",
        "The notion of equivariance is integral to an emerging research paradigm called **Geometric Deep Learning** -- systematically devising neural network architectures with inductive biases for leveraging symmetries in data. This interactive notebook aims to be a gentle introduction into the world of Geometric Deep Learning and distill unifying principle emerging in recent literature.\n",
        "\n",
        "We recommend opening a copy of this notebook via **Google Colab** (working locally is also easy):\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/chaitjo/geometric-gnn-dojo/blob/main/geometric_gnn_101.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "The **aims** of this notebook are as follows:\n",
        "\n",
        "* Motivate a gentle introduction to the concepts of **invariance** and **equivariance**, which are fundamental for modeling natural phenomena with known symmetries. We will cover theory and proofs and provide synthetic experiments to supplement our results\n",
        "* Become hands-on with [**PyTorch Geometric**](https://pytorch-geometric.readthedocs.io/en/latest/) (PyG), a popular libary for developing state-of-the-art GNNs and Geometric Deep Learning models. In particular, gaining familiarity with the `MessagePassing` base class for designing novel GNN layers and the `Data` object for representing graph datasets.\n",
        "* Gaining an appreciation of the fundamental principles behind constructing GNN layers that take advantage of **geometric information** for graphs embedded in **3D space**, such as biomolecules, materials, and other physical systems.\n",
        "\n",
        "## Authors and Acknowledgements\n",
        "**Authors**:\n",
        "\n",
        "- Clayton Curry (claycurry34@ou.edu)\n",
        "\n",
        "I take sole responsiblity for all errors, mischaracterizations, improper / missing citations, and all difficulties encountered by anyone who wishes to explore this fascinating area of research. \n",
        "\n",
        "\n",
        "**Acknowledgements**:\n",
        "\n",
        "The process of inventing and distilling new knowledge, by its very nature, involves looking only slightly farther than those who came before. In my own view, an academic work with anything less than abundant citations are a sheer denial of the opportunity to allow others to un ideas in the context that generated them. Whether in a slightly modified form or original to publication, if you notice your original idea appear without a proper citation, please allow me the opportunity to correct my error (which will also be cited) and give others the opportunity to stand on your original work.\n",
        "\n",
        "All intellectual credit in this notebook is rightfully owed to my peers in the deep learning and open source literature. Furthermore, several parts of this repository either adapt or borrow heavily from the authors cited in comments and in the acknowledgements.\n",
        "\n",
        "Lightning Equivariant would not exist if it wasn't for [Chaitanya K. Joshi's](https://www.chaitjo.com/) repository [Geometric GNN Dojo](https://github.com/chaitjo/geometric-gnn-dojo), which opened my eyes to the possiblity of making advanced literature more accessible without abstracting too far from the details. A handful of the modules in Lightning Equivariant were clean copied out of [Geometric GNN Dojo](https://github.com/chaitjo/geometric-gnn-dojo).\n",
        "\n",
        "and any interested reader is urged to subscribe to [Chaitanya K. Joshi's Website](https://www.chaitjo.com/) and repo [Geometric GNN Dojo](https://github.com/chaitjo/geometric-gnn-dojo). Other authors specifically listed as contributing to [Geometric GNN Dojo](https://github.com/chaitjo/geometric-gnn-dojo) include,\n",
        "\n",
        "- Chaitanya K. Joshi (ckj24@cl.cam.ac.uk)\n",
        "- Charlie Harris (cch57@cam.ac.uk)\n",
        "- Ramon Viñas Torné (rv340@cam.ac.uk)\n",
        "\n",
        "developed for students taking the following courses: [Representation Learning on Graphs and Networks](https://www.cl.cam.ac.uk/teaching/2122/L45), at University of Cambridge's Department of Computer Science and Technology (instructors: Prof. Pietro Liò, [Dr. Petar Veličković](https://petar-v.com/)), [Geometric Deep Learning](https://aimsammi.org/), at the African Master’s in Machine Intelligence (instructors: Prof. Michael Bronstein, Prof. Joan Bruna, Dr. Taco Cohen, Dr. Petar Veličković).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N0oFTe7ZiO8i"
      },
      "source": [
        "# ⚙️ Part 0: Installation and Setup\n",
        "\n",
        "**❗️Note:** You will need a GPU to complete this practical. If using Colab, remember to click `Runtime -> Change runtime type`, and set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY7foToFoo8Q"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mJupyter server crashed. Unable to connect. \n",
            "\u001b[1;31mError code from Jupyter: 1\n",
            "\u001b[1;31musage: jupyter.py [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
            "\u001b[1;31m                  [--paths] [--json] [--debug]\n",
            "\u001b[1;31m                  [subcommand]\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mJupyter: Interactive Computing\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mpositional arguments:\n",
            "\u001b[1;31m  subcommand     the subcommand to launch\n",
            "\u001b[1;31m\n",
            "\u001b[1;31moptions:\n",
            "\u001b[1;31m  -h, --help     show this help message and exit\n",
            "\u001b[1;31m  --version      show the versions of core jupyter packages and exit\n",
            "\u001b[1;31m  --config-dir   show Jupyter config dir\n",
            "\u001b[1;31m  --data-dir     show Jupyter data dir\n",
            "\u001b[1;31m  --runtime-dir  show Jupyter runtime dir\n",
            "\u001b[1;31m  --paths        show all Jupyter paths. Add --json for machine-readable\n",
            "\u001b[1;31m                 format.\n",
            "\u001b[1;31m  --json         output paths as machine-readable json\n",
            "\u001b[1;31m  --debug        output debug information about paths\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mAvailable subcommands:\n",
            "\u001b[1;31m\n",
            "\u001b[1;31mJupyter command `jupyter-notebook` not found. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# For storing experimental results over the course of the practical\n",
        "import pandas as pd\n",
        "RESULTS = {}\n",
        "DF_RESULTS = pd.DataFrame(columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O_e6GlqvRQn9"
      },
      "source": [
        "Great! We are ready to dive into the practical!\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Infrastructure\n",
        "\n",
        "This section covers the fundamentals. \n",
        "\n",
        "\n",
        "> **PyTorch** and consists of various methods for deep learning on graphs and other irregular structures, also known as Geometric Deep Learning, from a variety of published papers. In addition, it provides easy-to-use mini-batch loaders for operating on many small and single giant graphs, multi GPU-support, distributed graph learning, a large number of common benchmark datasets, and helpful transforms, both for learning on arbitrary graphs as well as on 3D meshes or point clouds.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Learning Theory\n",
        "\n",
        "## Learning from Data\n",
        "Learning algorithms descend to us as our greatest attempt to answer how reasoning may be modeled computationally. Constructing learning algorithms is fundamentally posed as solving a problem by induction. Unlike a traditional algorithm, consisting of one or several elementary steps for solving a fixed, concrete problem, instead, a learning algorithm $A$ is a sequence of elementary steps to solve the auxiliary task of *learning to “improve”* at some arbitrary task $T$ using arbitrary examples $X$, where “improvement” is with respect to some arbitrary measure of success $R$ (reward) or failure $L$ (loss). Put differently, given an arbitrary task $T$ and examples $X$, we wish to construct an algorithm $A$ that returns a hypothesis $h$ that solves best solves $T$ with respect to $L$. An important property of any learning algorithm $A$, a property of $A$ that motivates the bulk of this project, is the unique ***hypothesis class*** $\\mathcal H$ generated by $A$ where each element $h \\in \\mathcal H$, called a ***hypothesis*** produced by $A$ for the task $T$. This construction allows us to think of reward/loss as a mapping from hypotheses to reals with the form, \n",
        "\n",
        "$$\n",
        "L : \\mathcal H \\to \\mathbb R\n",
        "$$\n",
        "\n",
        "In the case of neural networks, every hypothesis $h$ is produced by some (not necessarily unique) set of weights $\\theta \\in \\mathbb{R}^n$ parameterizing a solution to $T$. In the literature, these two are commonly denoted by $h_\\theta$ or in functional notation $\\hat f_\\theta : \\mathcal X \\to \\mathcal Y : x \\mapsto \\hat y$. The popular ***gradient descent*** algorithm learns a good set of weights by incrementally adjusting $\\theta$ in the direction of steepest descent along the \"loss landscape\" given $L(w) \\in \\mathbb R$.\n",
        "\n",
        "From learning theory, we know that three fundamental limits exist on the ability of A to minimize L: statistical error, approximation error, and optimization error. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🧪 Experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_l0tU8gum9eT",
        "TAwoHyWs452X",
        "V03t1f51tW2w",
        "0fZTuGyVNC6n",
        "kY6J_vL3hgCN",
        "wqcJy1HtPgj6",
        "yO7iwRjvDD16",
        "c-az-clhTLLv",
        "04WVO1Mnukdm"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "maneuver",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:26:08) [Clang 14.0.6 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "1601518b5353712ad7eee8925ff031b748032cdc071a48a129452f6919a9b826"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
